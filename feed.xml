<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://deeliu97.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://deeliu97.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-23T15:31:23+00:00</updated><id>https://deeliu97.github.io/feed.xml</id><title type="html">Diyi Liu</title><entry><title type="html">Feedback on the Draft Delegated Regulation on Data Access Provided for in the Digital Services Act</title><link href="https://deeliu97.github.io/blog/2024/oii-dsa40/" rel="alternate" type="text/html" title="Feedback on the Draft Delegated Regulation on Data Access Provided for in the Digital Services Act"/><published>2024-12-10T14:14:00+00:00</published><updated>2024-12-10T14:14:00+00:00</updated><id>https://deeliu97.github.io/blog/2024/oii-dsa40</id><content type="html" xml:base="https://deeliu97.github.io/blog/2024/oii-dsa40/"><![CDATA[<p>Written by Diyi Liu, Manuel Tonneau and Juliette Zaccour</p> <p>The full submitted feedback form can be accessed <a href="https://ec.europa.eu/info/law/better-regulation/have-your-say/initiatives/13817-Delegated-Regulation-on-data-access-provided-for-in-the-Digital-Services-Act/F3498917_en">here</a>. A shorter blog piece was published in the <a href="https://www.oii.ox.ac.uk/news-events/oii-researchers-propose-recommendations-for-effective-data-governance-in-light-of-the-eus-digital-service-act/">Oxford Internet Institute Blog Website</a>.</p> <p>We are doctoral researchers from the Oxford Internet Institute, University of Oxford, specialising in the technical, ethical, and socio-legal dimensions of platform governance. Our research focuses on empirically investigating systemic risks posed by digital platforms, with particular emphasis on auditing human-in-the-loop content moderation systems. Our research expertise includes both the effectiveness of platforms’ approaches to illegal and harmful content, and the broader implications of data access mechanisms for public interest research. We also conduct research on privacy-preserving data sharing solutions and their impact on research integrity and reliability.</p> <p>As individual researchers actively engaged in platform and data governance studies, we offer this feedback on the proposed mechanisms and practical implications of the Draft Delegated Regulation on Data Access under the Digital Services Act (hereinafter the Draft Act).</p> <p>The Draft Act emerges at a critical inflection point for platform research. Recent developments have created significant obstacles to independent investigation of platform governance and societal impact. These challenges include the widespread shutdown of platform APIs and research tools , severely limiting researchers’ ability to study systemic risks arising from digital platforms . More concerning still, independent researchers face increasing legal pressures when conducting studies related to harmful content and(or) algorithms on certain platforms . These developments underscore the urgent need for robust regulatory frameworks ensuring protected platform data access for research. In this context, Article 40 of the DSA represents not only a crucial mechanism for ensuring platform accountability within the European Union, but also establishes a potential global standard for researcher access to platform data.</p> <p>In this feedback letter, we address several aspects of the Draft Act requiring clarification before finalisation. Our analysis focuses on three key areas: the appropriateness of data access modalities, the scope and context of accessible data, and the underlying enforcement and coordination mechanisms. Drawing on our empirical research experience, we offer recommendations for strengthening these provisions to ensure meaningful transparency and effective implementation.</p> <h3 id="data-access-modalities-and-conditions">Data Access Modalities and Conditions</h3> <p>The determination of data access modalities outlined in Article 9 of the Draft Act requires substantial clarification. While Recital 6 requires data providers to provide data inventory overviews and “where possible, indicate suggested modalities for accessing them,” Recital 16 requires applicant researchers to propose preferred access modalities in data access application. This creates potential confusion during the decision-making process for determining appropriate access modalities and the rationale behind such decisions.</p> <p>Moreover, the current framework may inadvertently encourage researchers to accept whatever access modalities are available in the inventory, which could include Terms of Service from data providers or third-party providers that impose additional restrictions. While Article 15(3) prohibits data providers from imposing “archiving, storage, refresh and deletion requirements that hinder the research referred to in the reasoned request in any way,” stronger safeguards are needed for research independence. Data providers often include requirements in their Terms of Service for researchers to submit research outputs before publication, ostensibly to identify potential personal data disclosure. While data protection is crucial, the Draft Act should explicitly protect researchers’ academic freedom to publish findings without prior approval from the data providers. Specifically, data protection measures are to be established through the reasoned request; as Recital 16 suggests, it is the Digital Services Coordinator (DSC)’s role to verify that the access fulfils both data protection and research integrity requirements. Data providers have an opportunity to raise data protection concerns through the dispute settlement procedure (Article 13) and should therefore not intervene at the later stages of the research, so as to preserve researchers’ independence.</p> <p><strong>We recommend that the Draft Act establishes baseline requirements for appropriate data access modalities, developed through expert consultation during the finalisation of the Draft Act, to guide DSCs in evaluating proposed access arrangements. Additionally, the Draft Act should explicitly address any types of data sharing agreements that impose post-access restrictions on research independence, particularly regarding the publication of research findings. Data protection concerns should be addressed through the established procedures in Article 13 rather than through other provider-imposed restrictions.</strong></p> <h3 id="scope-of-data-access-and-meaningful-transparency">Scope of Data Access and Meaningful Transparency</h3> <p>The Draft Act provides a comprehensive framework in Recital 12 for data access through reasoned requests, particularly “data related to content moderation and governance, such as data on algorithmic or other content moderation systems and processes, archives or repositories documenting moderated content, including accounts as well as data related to prices, quantities and characteristics of goods or services provided by the data provider.” While the Draft Act rightfully emphasizes necessity and proportionality in data access applications, its current implementation framework presents significant challenges.</p> <p>The requirement for researchers to prove that their research purposes “cannot be achieved by other existing means” creates an undue burden of proof, particularly regarding data that are deemed available “through other sources” (Recital 12). In practice, this could allow platforms to deflect legitimate research requests by pointing to existing transparency disclosures, even when such data lacks the granularity or context needed for meaningful research on systemic risks.</p> <p>Our ongoing analysis of the DSA transparency reports from Very Large Online Platforms (VLOPs), particularly regarding content moderation workforce and effectiveness, demonstrates the limitations in both scope and context of data available through current transparency practices.</p> <p>Regarding the moderation workforce, the lack of appropriate normalisation metrics severely hinders meaningful analysis. While the number of moderators is reported by EU language, there is no accompanying information on the volume of content generated in each language, making it impossible to evaluate whether the allocated resources are sufficient for each EU language. Platforms do provide data on monthly average user numbers by EU country, but this data is an imperfect proxy for normalisation due to two key issues. First, languages often span multiple countries, and individual countries may have speakers of various languages, making it hard to map country-level user numbers to language-level moderator numbers. Second, moderators primarily review content rather than users, meaning that user numbers provide an imperfect insight into the actual workload for content moderation. Without robust language-specific content volume metrics, assessing the adequacy of language-level moderator numbers therefore remains highly problematic, despite platforms having access to the necessary data for more robust assessments of resource allocation.</p> <p>Beyond the question of missing normalisation data, we argue that more justifications are needed from platforms to motivate potential cross-lingual disparities in their moderator workforce. Our ongoing work reveals significant disparities in resources invested by platforms in moderator workforce across EU languages when normalising by academic estimations of the amount of content that moderators need to review. While there may be legitimate reasons for such disparities – such as different performance in AI detection models used for moderation across languages, different prevalence rates of harmful content, or financial incentives and operational constraints across markets – platforms generally provide no justification for these resource allocations. Understanding these rationales is crucial for contextualising whether moderation resources are appropriately distributed to address systemic risks across the Union.</p> <p>We encounter similar issues when studying moderation enforcement: while platforms report absolute numbers of moderated harmful content for categories such as hate speech and child sexual abuse materials (CSAM), these figures lack baseline data on total volume of harmful content that would help understand the share of all harmful content that is actually moderated. Although platforms conduct internal research on the prevalence and effectiveness of content moderation, such as Facebook’s studies on hate speech prevalence based on representative sampling , there are no mechanisms in the current Draft Act to ensure independent researchers can access such analytical data for verification and further study. In the absence of data sharing from platforms on the subject, getting access to such information can prove prohibitively costly for researchers as illustrated by our recent work : to examine how the prevalence and composition of hate speech varies across eight languages and four English-speaking countries, we annotated a representative sample of tweets posted within one day on Twitter (now X) . The annotation costs were approximately 30,000EUR, which shows the substantial barriers researchers face when platforms withhold contextual data necessary for analysis of systematic risks, particularly the significant resources required to replicate analysis that platforms conduct internally.</p> <p><strong>To address these challenges and ensure meaningful transparency while respecting the principles of necessity and proportionality outlined in the Draft Act, we recommend that the Draft Act: (a) clarify the burden of proof for data access requests by establishing clear criteria for what constitutes adequate alternative data sources, (b) require DSCs of establishment to maintain an updated registry of available data sources, and (c) explicitly address the scope of accessible data to include necessary contextual information for raw numbers. Continuing with the example above, necessary contextual data could include moderator count per language, both the total volume of harmful content (prevalence) and the volume that is actually detected and moderated (enforcement), which could strengthen our assessment of moderation effectiveness.</strong></p> <h3 id="balancing-data-protection-with-research-integrity">Balancing Data Protection with Research Integrity</h3> <p>While Recital 16 states that the DSC “should assess whether the access modalities proposed by the applicant researchers in the data access application are appropriate to fulfil the requirements of data security, data confidentiality and protection of personal data and, at the same time, enable the attainment of the research objectives of the research project”, in making decisions on the appropriate access modalities, Article 9(2) limits the DSC’s considerations to “the sensitivity of the data requested, the rights and interests of the data provider, including the protection of confidential information, in particular trade secrets, and the security of its service”. Without explicitly stating the need to balance these considerations with research needs and the public interest, there is a risk of undermining the social value of the proposed research.</p> <p><strong>We recommend amending Article 9(2) to clarify the DSCs should balance the risks and benefits of data access in deciding on appropriate access modalities, by considering not only data sensitivity and provider interests but also the value of the proposed research and its alignment with the public interest.</strong></p> <p>Previous research has documented instances where platform-provided data or research tools proved inconsistent or misleading . To address these concerns, the Draft Act should require data completeness and quality assurances from the data providers. Without these additional safeguards and specific requirements, the risk of “transparency theatre” – where platforms provide data that satisfy technical requirements without enabling meaningful oversight – remains significant . The Draft Act presents an opportunity to establish not only legal requirements for data access, but also comprehensive standards for meaningful access that enable effective research and accountability.</p> <p><strong>We therefore recommend adding provisions that require data providers to attest to the completeness, accuracy, and representativeness of the shared data. This would ensure that access modalities go beyond formal compliance and effectively enable meaningful research.</strong></p> <p>Indeed, beyond the modalities of access, the granularity and quality of the data itself are determinant in establishing meaningful access. In particular, the choice of anonymisation techniques is critical in ensuring research integrity while safeguarding privacy . Unilateral decisions by the data providers related to data anonymisation, such as using privacy-enhancing technologies like differential privacy and synthetic data generation, hold a risk of (voluntarily or involuntarily) significantly altering the data and undermining the reliability and validity of research findings .</p> <p><strong>Therefore, we recommend (a) amending Article 9 to clarify the extent to which each party is responsible for determining appropriate anonymization techniques (where relevant), and (b) amending Article 15 to require that data providers adequately document the use of data processing and anonymization techniques, and provide validity guarantees to researchers. By extension, we recommend extending Article 15(3) to specify that data providers should not impose data processing and aggregation that was not set out by the reasoned request and that hinder the research in any way.</strong></p> <h3 id="coordination-mechanisms">Coordination Mechanisms</h3> <p>The Draft Act’s emphasis on harmonisation and consistency across DSCs of establishment raises concerns regarding implementation and oversight. While assigning substantial responsibility to DSCs of establishment and other national regulators, the Draft Act lacks robust mechanisms for ensuring consistent application of standards across jurisdictions. A primary concern lies in the handling of delays and determination of access modalities. Recital 7 requires DSCs of establishment to notify principal researchers of delays in processing reasoned requests, particularly when “data access applications imply international data transfers” or where risks to “security of the Union” are detected. Furthermore, Article 9(2) grants DSCs considerable discretion in determining the appropriateness of access modalities, requiring consideration of “the sensitivity of the data requested, the rights and interests of the data provider, including the protection of confidential information, in particular trade secrets, and the security of its service.” Without specific guidance, this could lead to inconsistent interpretations and applications across jurisdictions, service providers, and individual requests.</p> <p><strong>We recommend developing detailed criteria and guidelines for applying these exemptions to ensure the implementation of these provisions across DSCs.</strong></p> <h3 id="dispute-resolution">Dispute Resolution</h3> <p>Article 13 of the Draft Act outlines provisions for dispute settlement and mediation following amendment requests, which present several procedural concerns. The Draft Act leaves critical aspects of the mediation process undefined, which raises questions about its effectiveness and fairness.</p> <p>First, data providers are required to propose mediators when initiating the mediation (Article 13(3)), and “shall be solely responsible for covering the costs of the mediation” (Article 13(4)). This raises concerns about whether mediators can be impartial and independent if they are both appointed and funded by data providers while being expected to balance provider and researcher interests in the mediation.</p> <p>The Draft Act doesn’t specify what constitutes “undue delay” in the mediation process. More critically, Article 13(8) states that if no agreement is reached by the time limit set by the DSCs, the mediator shall declare the mediation closed, but remains unclear on the subsequent course of actions. This creates uncertainty about whether data access should proceed as specified in the original reasoned requests or be denied entirely.</p> <p><strong>We recommend that the Draft Act clarifies the appointment procedure for mediators, the scope of mediator authority, the binding nature of mediator decisions, and the default outcome when mediation closes without agreement.</strong></p> <p>More concerning is the limited role granted to affected researchers in the mediation process. Article 13(5) states that principal researchers will be invited to join mediation only “where appropriate”. This conditional involvement suggests researchers may have minimal influence during dispute resolution, despite being directly affected by outcomes.</p> <p><strong>We recommend that the Draft Act explicitly define circumstances requiring researcher participation and establish clear protocols for keeping researchers informed throughout the mediation process.</strong></p> <h3 id="vetting-process-and-potential-impacts-on-fair-representation">Vetting Process and Potential Impacts on Fair Representation</h3> <p>Lastly, the Draft Act, while advancing crucial data access provisions, requires careful consideration of its potential downstream impacts on academic research and knowledge production. The vetting process and institutional requirements could disproportionately advantage well-resourced research institutions, particularly those within the EU. This raises important questions about equitable access to platform data and its implications for global knowledge production (e.g., Article 9 (4)(d) sets the condition “that the computing power at the disposal of the vetted researchers is appropriate and sufficient for the purposes of the research project”). This could concentrate platform research within a select group of institutions, potentially limiting diverse perspectives and approaches . Without careful consideration of non-EU researcher access, the Act might inadvertently create a two-tier system of platform research. Previous written evidence submitted by colleagues at the Oxford Internet Institute also discussed data access outside the EU . The issue is particularly concerning given that platform impacts and risks often transcend geographical boundaries, requiring diverse global perspectives for comprehensive understanding of systemic risks.</p> <p><strong>We recommend that the Draft Act incorporate specific provisions to (a) ensure technical and infrastructural requirements do not create unnecessary barriers to entry for smaller institutions, (b) create clear guidelines for international research collaboration or consider the establishment of shared research infrastructure for researchers from non-EU institutions.</strong></p> <h3 id="references">References</h3> <p>Perriam, Jessamy, Andreas Birkbak, and Andy Freeman. “Digital methods in a post-API environment.” International Journal of Social Research Methodology 23, no. 3 (2020): 277-290.</p> <p>Davidson, Brittany I., Darja Wischerath, Daniel Racek, Douglas A. Parry, Emily Godwin, Joanne Hinds, Dirk van der Linden, Jonathan F. Roscoe, Laura Ayravainen, and Alicia G. Cork. “Platform-controlled social media APIs threaten open science.” Nature Human Behaviour 7, no. 12 (2023): 2054-2057</p> <p>Loveluck, L. “How new Twitter rules could hinder war crimes research and rescue efforts.” The Washington Post. June 20, 2023, accessed Dec 01, 2024, https://www.washingtonpost.com/technology/2023/06/20/twitter-policy-elon-musk-api/;</p> <p>“Letter: Twitter’s New API Plans Will Devastate Public Interest Research.” Coalition for Independent Technology Research. Apr 3,2023, accessed Dec 01, 2024, https://independenttechresearch.org/letter-twitters-new-api-plans-will-devastate-public-interest-research/.</p> <p>Nick Robins-Early. “Judge dismisses ‘vapid’ Elon Musk lawsuit against group that catalogued racist content on X.” The Guardian. Mar 25, 2024, accessed Dec 01, 2024, https://www.theguardian.com/technology/2024/mar/25/elon-musk-hate-speech-lawsuit</p> <p>Kayser-Bril, N. “AlgorithmWatch forced to shut down Instagram monitoring project after threats from Facebook.” AlgorithmWatch. Aug 13, 2021, accessed Dec 01, 2024, https://algorithmwatch.org/en/instagram-research-shut-down-by-facebook/</p> <p>Arcadiy Kantor. “Measuring Our Progress Combating Hate Speech.” Meta Newsroom, Nov 19, 2020, accessed Dec 05, 2024, https://about.fb.com/news/2020/11/measuring-progress-combating-hate-speech/</p> <p>Tonneau, Manuel, Diyi Liu, Niyati Malhotra, Scott A. Hale, Samuel P. Fraiberger, Victor Orozco-Olvera, and Paul Röttger. “HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter.” arXiv preprint arXiv:2411.15462 (2024).</p> <p>Pfeffer, Juergen, Daniel Matter, Kokil Jaidka, Onur Varol, Afra Mashhadi, Jana Lasser, Dennis Assenmacher et al. “Just another day on Twitter: a complete 24 hours of Twitter data.” Proceedings of the international AAAI conference on web and social media no.17 (2023): 1073-1081.</p> <p>Pearson, George DH, Nathan A. Silver, Jessica Y. Robinson, Mona Azadi, Barbara A. Schillo, and Jennifer M. Kreslake. “Beyond the margin of error: a systematic and replicable audit of the TikTok research API.” Information, Communication &amp; Society (2024): 1-19</p> <p>Timberg, C. “Facebook made big mistake in data it provided to researchers, undermining academic work.” The Washington Post. Sep 10, 2021, accessed Dec 01, 2024, https://www.washingtonpost.com/technology/2021/09/10/facebook-error-data-social-scientists/.</p> <p>Suzor, Nicolas P., Sarah Myers West, Andrew Quodling, and Jillian York. “What do we mean when we talk about transparency? Toward meaningful transparency in commercial content moderation.” International Journal of Communication 13 (2019): 18.</p> <p>Gadotti, Andrea, Luc Rocher, Florimond Houssiau, Ana-Maria Creţu, and Yves-Alexandre de Montjoye. “Anonymization: The imperfect science of using data while preserving privacy.” Science Advances 10, no. 29 (2024): eadn7053.</p> <p>Stadler, Theresa, and Carmela Troncoso. “Why the search for a privacy-preserving data sharing mechanism is failing.” Nature Computational Science 2, no. 4 (2022): 208-210</p> <p>Hauer, Mathew E., and Alexis R. Santos-Lozada. “Differential privacy in the 2020 census will distort COVID-19 rates.” Socius 7 (2021); Offenhuber, Dietmar. “Shapes and frictions of synthetic data.” Big Data &amp; Society 11, no. 2 (2024)</p> <p>Nagaraj, Abhishek, Esther Shears, and Mathijs de Vaan. “Improving data access democratizes and diversifies science.” Proceedings of the National Academy of Sciences 117, no. 38 (2020): 23490-23498.</p> <p>Ibrahim, Lujain, Rocher Luc., and Valdivia, Ana. “Oxford experts call for better access to platform data.” Oxford Internet Institute, June 8, 2023, accessed Dec 01, 2024, https://www.oii.ox.ac.uk/news-events/oxford-experts-call-for-greater-access-to-platform-data/.</p>]]></content><author><name></name></author><category term="policy"/><category term="brief"/><category term="dataaccess"/><category term="dsa"/><summary type="html"><![CDATA[Submitted feedback form for effective data access and data governance in light of the EU’s Digital Service Act 40.]]></summary></entry><entry><title type="html">Zooming in on the Digital Aspects of the Indonesian Elections 2024</title><link href="https://deeliu97.github.io/blog/2024/oii-indo-election/" rel="alternate" type="text/html" title="Zooming in on the Digital Aspects of the Indonesian Elections 2024"/><published>2024-02-09T14:14:00+00:00</published><updated>2024-02-09T14:14:00+00:00</updated><id>https://deeliu97.github.io/blog/2024/oii-indo-election</id><content type="html" xml:base="https://deeliu97.github.io/blog/2024/oii-indo-election/"><![CDATA[<p>Written by Rizal Shidiq, Diyi Liu and Justin Yeung</p> <p>Originally published in the <a href="https://www.oii.ox.ac.uk/news-events/zooming-in-on-the-digital-aspects-of-the-indonesian-elections-2024/">Oxford Internet Institute Blog Website</a>.</p> <h3 id="the-context-and-relevance-of-the-2024-indonesian-election">The context and relevance of the 2024 Indonesian election</h3> <p>On Valentine’s Day, 14th February, 2024, Indonesia will hold its fifth direct presidential election since transitioning to democracy in 1998. Over 204 million registered voters across the archipelago will flock to polling stations to cast their votes from over 300,000 candidates vying for over 20,000 national and local legislative seats.</p> <p>Indonesia employs an open-list proportional representation system for its multiparty parliamentary elections, while the presidency is elected by a national first-past-the-post popular vote. With current President Joko Widodo (Jokowi) reaching term limits, recent polls suggest a competitive contest among three candidates (The Economist, 2024). Leading is the incumbent defence minister Prabowo Subianto, running with 36-year-old Gibran Rakabuming Raka, current president Jokowi’s eldest son, in a big tent bid to galvanise establishment backers alongside populists and conservative Muslims. Next is former Jakarta Governor and education minister Anies Baswedan with Muhaimin Iskandar, head of Islam-based National Awakening Party, mainly targeting urban middle class voters. Finally, comes the popular technocratic and former Central Java Governor Ganjar Pranowo, paired with Mahfud MD, current coordinating minister for politics, legal, and security affairs, aiming to gain support from the populous Muslim majority (see The Jakarta Post for detailed candidate profiles).</p> <p>Voters will cast ballots for not only the presidency, but also for seats in the People’s Consultative Assembly and Regional Representative Council that comprise the national legislature, as well as provincial governors and district/municipal legislative bodies. Each ticket represents established political blocs, though most Indonesian parties comprise heterogeneous elements rather than solely ideological bases.</p> <h3 id="digital-media-election-integrity-and-democracy">Digital Media, Election Integrity, and Democracy</h3> <p>The 2024 election represents a critical test for Indonesian democracy as a young, predominantly Muslim nation. Indonesia is classified as a partly free democracy: while the country has held free and fair elections with peaceful transitions of power since the end of the authoritarian regime in 1998, civil liberties can be limited due to e.g. the discrimination of minority groups or the politicised use of blasphemy laws (Freedom House, 2023). For some years, the country has witnessed democratic backsliding through systematic state efforts to weaken opposition groups and concentrate power (Power, 2018). More recently, the political manoeuvring in the high-level Constitutional Court enabled Gibran Rakabuming Raka, the incumbent president’s son, to controversially run for vice president (for Prabowo, Jokowi’s previous-two elections rival), underscoring worries that Indonesian democracy is creeping towards dynastic politics (Paddock and Suhartono, 2024). Adding to these concerns around fair democratic processes are the increasing threats from online spaces – where false information, secretive computational propaganda campaigns and unchecked digital advertising increasingly undermine election integrity.</p> <p>Digital platforms have become fiercely contested spaces integral to the country’s democratic discourse. With over 66% of Indonesians now online as internet penetration expands, digital spaces are poised to shape the 2024 polls even more profoundly than in the past (BPS-Statistics Indonesia, 2023). Nearly half the country’s internet users fall between the key voting ages of 25-46, while tech-savvy millennials and Gen Z comprise over 50% of registered voters. Indonesians really love the internet and social media: they spend 7 hours 42 minutes on the internet daily – out of which 3 hours 18 minutes are on social media (Statista, 2023). While this has enabled more interactive, participatory, and personalised political campaigning, the serious dangers of hate speech, ethnic and religious polarisation and orchestrated viral mis/disinformation become severe. While not a completely new phenomenon since it had been started in the 2012 Jakarta governor election (Sastraamidjaja, Rasidi, &amp; Elsitra, 2022), the growing potential disinformation and political polarisation coming from the buzzers’ activities in the Indonesian political online sphere is alarming, as seen in the 2014 and 2019 elections (Hui, 2020) and recent anti-Rohingya campaigns (Ratcliffe, 2024).</p> <p>On the other hand, higher exposure to social media also means a new way for the candidates to reach voters. In doing so, they reveal the signals about themselves (their identities) and, perhaps inadvertently, their political alliances (their connections). All presidential candidates, and to some extent parliamentary candidates, are very active in social media platforms to woo voters. For example, Anies-Muhaimin, and later on Ganjar-Mahfud, have held a well-attended series of live interactive TikTok forums. A bit of a caveat on social media effects, though: voters still rely on TV as their main sources of information on social and political issues (53 percent). Only 26 percent get the political info from Tiktok and merely 7.2 percent from X (formerly Twitter) (Indikator, 2024. Note: respondents can indicate more than one media source). Thus, online campaigns might not really work for quite a significant number of voters.</p> <p>Not wanting to miss out on the trend, AI is also becoming more popular for the candidates. For example, to soften his image as a former military man and allegedly a human rights abuser, Prabowo camp deliberately uses AI to create his and his running mates’ images into cartoons! Anies and Ganjar camps also launch AI (or digital) platforms – i.e., Haveaniesday.com and https://team.ganjar.ai/. So far, the candidates have used AI for visual image manipulation. Whether they also use it for more sophisticated voter profiling, targeting, and manipulating is still unknown at this point.</p> <h3 id="visions-of-the-presidential-candidates-on-innovation-and-technology">Visions of the Presidential Candidates on Innovation and Technology</h3> <p>Looking beyond social media’s double-edged impacts, the leading presidential candidates have outlined policy visions speaking to Indonesia’s digital transformation and development. Taking the cue from their official (short) vision statement, Prabowo throws the words “digital economy” as part of a national security and resiliency agenda and “technology” as part of a human resource development agenda. Ganjar wants Indonesia to excel in science and “technology” and “digital ecosystem” by prioritising high-speed and affordable “internet”. Of course, the notion of digital and/or technology can be found in the extended versions of all candidates’ proposals and conversations (see Purwanto, 2023 and CNN Indonesia, 2024), but whether they make it into the main vision statement may tell something about the candidates’ priority.</p> <p>All candidates share the same concern on the digital economy, with some variation on its links with small and medium enterprises, creative economy, and infrastructure quality. For instance, Anies and Ganjar also seem to open the possibility to revise the controversial Law on Electronic Information and Transactions (UU ITE), that is oftentimes used to silence the political opposition, disguised as defamation and blasphemy in social media.</p> <p>All being said, beyond the papers and rhetoric, there is no credible signal that the candidates, once in power, would put consequential digital technology reform into action.</p> <h3 id="navigating-the-digital-landscape-multistakeholder-approaches-to-understand-the-2024-election">Navigating the Digital Landscape: Multistakeholder Approaches to Understand the 2024 Election</h3> <p>To champion meaningful reforms and regulations aimed at promoting more accountable, rights-respecting digital governance, stakeholders across various sectors are actively navigating the digital landscapes surrounding the 2024 election. From academic initiatives to governmental frameworks, as well as collaborative efforts with digital platforms, these endeavours collectively contribute to the pursuit of electoral integrity and informed civic participation.</p> <p>In November 2023, the Indonesian General Election Commission (KPU) released the official list of candidates with their short profile or curriculum vitae (CV). In this light, the Leiden Humanities AI and NLP Lab (Leiden HumAN) and the Leiden Institute of Area Studies (LIAS) launched the Political Social Network in Indonesia (POLISONI) project to collect CVs and images of over 300, 000 candidates. Thanks to the publicly accessible digital platform offered by the Election Commission, the project creates a large-scale, multimodal and open-access database of aspiring politicians in the context of Indonesia to allow for studies on Indonesian politics.</p> <p>Preliminary analysis revealed that only roughly 80% of the full profiles are publicly available. In the coming months, the project will utilise this dataset to expand the study of politics in Indonesia beyond national level candidates, but also the provincial and district level political environments. More importantly, with the help of the profile data, it will apply social network analysis to map out how politicians are connected with one another in terms of previous education, employment and familial ties. It will also shed light on how candidates’ self-representation relates to their political positions, religious affiliations and electability through analysis of the text (i.e. platform and slogans) and image data (i.e. profile pictures) in the database.</p> <p>Governmental initiatives like the Election Supervisory Agency’s (Bawaslu) multidimensional Election Vulnerability Index (IKP) now enable granular mapping of socio-political risks to pre-emptively safeguard electoral integrity. Major platforms like Meta and TikTok have also responded by implementing special election designated content moderation policies, yet consistently identifying and addressing trolling and harassment continues to pose challenges. Considering this, Indonesian civil society groups, multilateral organisations, along with Election Management Bodies (EMBs), have been urging technology platforms to embrace greater transparency and accountability in their content moderation policies and practices to protect election integrity. A major development occurred in January 2024 when major platforms signed on to the DAMAI Coalition and UNESCO’s social media for Peace Project joint commitments to fight disinformation, identity discrimination and hate speech, protect children, ensure transparency of political advertising, and guarantee access to platform data for independent research and monitoring.</p> <p>As the official election campaign period concludes on 10th February, the last days ahead of polls will certainly be interesting from a political as well as from an Internet research perspective. After the election, it remains to be seen how the new government follows through on important technology policy rhetoric as leader of the world’s third largest democracy and Southeast Asia’s largest economy. The 2024 race spotlights how the country is balancing the promise and perils of continued digital transformation and an increasingly digital public sphere.</p> <h3 id="references">References</h3> <p>To gain greater perspective on the policies and stakeholders governing online material in Indonesia: https://pr2media.or.id/publikasi/research-paper-regulations-for-moderating-illegal-content-on-social-media-in-indonesia/</p> <p>BPS-Statistics Indonesia. (2023). Telecommunication Statistics Indonesia, 2022. Jakarta: BPS-Statistics Indonesia</p> <p>CNN Indonesia. (2023). ‘Adu Strategi Program Ekonomi Digital Ala Anies, Prabowo dan Ganjar’. Available at: https://www.cnnindonesia.com/ekonomi/20231219153059-532-1039244/adu-strategi-program-ekonomi-digital-ala-anies-prabowo-dan-ganjar. (Accessed: 27 January 2024)</p> <p>Freedom House. (2023a). Indonesia: Freedom in the World 2023 Country Report. Freedom House. https://freedomhouse.org/country/indonesia/freedom-world/2023 (Accessed: 05 February 2024)</p> <p>Freedom House. (2023b). Indonesia: Freedom on the Net 2023 Country Report. Freedom House. https://freedomhouse.org/country/indonesia/freedom-net/2023 (Accessed: 05 February 2024)</p> <p>Indikator. (2024). Efek Elektoral Debat Capres: Perbandingan Temuan Survei Tatap Muka dan Survei Telepon. Available at: https://indikator.co.id/wp-content/uploads/2024/01/RILIS-INDIKATOR-20-JANUARI-2024-1.pdf). (Accessed: 26 January 2024).</p> <p>Hui, J. Y. (2020). Social Media and the 2019 Indonesian Elections. Southeast Asian Affairs, 155-172.</p> <p>Paddock, R.C. and Suhartono, M. (2024). ‘A President’s Son Is in Indonesia’s Election Picture. Is It Democracy or Dynasty’. New York Times, 6 January. Available at: https://www.nytimes.com/2024/01/06/world/asia/indonesia-presidential-election-dynasty.html (Accessed: 26 January 2024).</p> <p>Purwanto, A. (2023). ‘Menakar Arah Transformasi Ekonomi Digital Para Calon Pemimpin’. Kompas, 23 December. Available at https://www.kompas.id/baca/riset/2023/12/23/menakar-arah-transformasi-ekonomi-digital-para-calon-pemimpin (Accessed, 27 January 2024).</p> <p>Ratcliffe, R. (2024). The online hate campaign turning Indonesians against Rohingya refugees. The Guardian, 18 January. Available at: https://www.theguardian.com/world/2024/jan/18/the-online-hate-campaign-turning-indonesians-against-rohingya-refugees</p> <p>Sastramidjaja, Y., Rasidi, P.P., &amp; Elsitra, G.N. (2022). Peddling Secrecy in a Climate of Distrust: Buzzers, Rumours and Implications for Indonesia’s 2024 Elections. Perspective, Singapore: ISEAS Yusof Ishak Institute</p> <p>Statista. (2023). ‘Average daily time spent using various media and devices in Indonesia in 3rd quarter 2022, by activity’. (Accessed: 26 January 2024).</p> <p>The Economist. (2024). Who will be the next president of Indonesia? Available at: https://www.economist.com/interactive/2024-indonesia-election-tracker. (Accessed: 04 February 2024)</p> <p>Thomas P. Power (2018) Jokowi’s Authoritarian Turn and Indonesia’s Democratic Decline, Bulletin of Indonesian Economic Studies, 54:3, 307-338, DOI: 10.1080/00074918.2018.1549918</p>]]></content><author><name></name></author><category term="commentaries"/><category term="election"/><category term="indonesia"/><summary type="html"><![CDATA[On 14th February 2024, Indonesia will hold its fifth direct presidential election since transitioning to democracy in 1998.]]></summary></entry><entry><title type="html">Internet For Trust Towards a Multistakeholder Approach in Regulating Digital Platforms</title><link href="https://deeliu97.github.io/blog/2023/oii-internet-for-trust/" rel="alternate" type="text/html" title="Internet For Trust Towards a Multistakeholder Approach in Regulating Digital Platforms"/><published>2023-07-17T14:14:00+00:00</published><updated>2023-07-17T14:14:00+00:00</updated><id>https://deeliu97.github.io/blog/2023/oii-internet-for-trust</id><content type="html" xml:base="https://deeliu97.github.io/blog/2023/oii-internet-for-trust/"><![CDATA[<p>Originally published in the <a href="https://www.oii.ox.ac.uk/news-events/internet-for-trust-towards-a-multistakeholder-approach-in-regulating-digital-platforms/">Oxford Internet Institute Blog Website</a>.</p> <h3 id="regulating-speech-in-the-age-of-digital-platforms">Regulating speech in the age of digital platforms</h3> <p>Digital platforms have become increasingly influential in enabling and constraining information flow in the global public sphere, shaping what we see and how we see the world. Since September 2022, the UNESCO team has been working on developing the <a href="https://unesdoc.unesco.org/ark:/48223/pf0000384031.locale=en">Guidelines for Regulating Digital Platforms</a> under its global mandate that includes the promotion of “the free flow of ideas by word and image”. The aim of the Guidelines is to safeguard freedom of expression, access to information, and other human rights in the context of the development and implementation of digital platform regulatory processes. It argues that such regulatory processes should be led through an open, transparent, multistakeholder, proportional and evidence-based manner.</p> <p>On June 20th, researchers at the Oxford Internet Institute hosted an independent community consultation to stress test the current draft of the UNESCO Guidelines to rigorous examination and evaluation. This community session serves as part of <a href="https://www.unesco.org/en/internet-conference/guidelines-consultation-process">a wider global consultation effort</a> that seeks to ensure opportunities for inclusive participation in the Guidelines drafting-process.</p> <p>The consultation brought together researchers from various stakeholder groups (academia, civil society, government, and the technical community) who covered diverse research interests, ranging from human rights law, online extremism, to deepfake and generative AI, deceptive design, as well as ICT for development (ICT4D). The community also represented invaluable global perspectives including the UK/EEA, the US, Latin America and Asia-Pacific as well as South Africa. Using the Guidelines as a practical anchor point, the session served as a common ground for researchers to engage in meaningful interdisciplinary discussions.</p> <p>The UNESCO Guidelines specifies five key principles that digital platforms should comply with:</p> <ul> <li>Platforms conduct human rights due diligence, evaluating the risks and impact on human rights of their policies and practices as well as defining the mitigation measures.</li> <li>Platforms should adhere to International Human Rights Standards, including in platform design, content moderation and curation.</li> <li>Platforms are transparent, being open about how they operate, with understandable and auditable policies as well as multi-stakeholder-agree metrics for evaluating performance.</li> <li>Platforms make available information and tools for users</li> <li>Platforms are accountable to relevant stakeholders</li> </ul> <p>With these five principles in mind, this blog summarised some of the new considerations that emerged regarding certain aspects in the current Guidelines where there was no consensus during previous consultations.</p> <h3 id="scoping">Scoping</h3> <p>At the centre of the discussion was the key question, ‘What types of digital platforms should be included in the scope of the Guidelines? Or broadly speaking, which level(s) of the digital system, and which aspects of them are we referring to when discussing the issue of content moderation (e.g., risk-based, size and market share, or functionality)?</p> <p>The current Guidelines specifically focuses on user to user services (i.e., any internet service which allows users to generate, upload or share content online) and search services, targeting large platforms, particularly market dominant platforms. The rationale behind the size and market share approach was well acknowledged: platforms in different scales possess different capacity (and incentives in the first place) of investing in content moderation, and digital monopoly should be taken into account when it comes to regulatory frameworks.</p> <p>Most participants agreed that we need to strike a balance between overuse and underuse, ensuring that the regulatory approach is neither overly permissive nor excessively restrictive, so as not to harm human rights or otherwise impede innovation. One way to achieve this balance might be to adopt a tiered and progressive approach that can effectively address different levels of risks associated with different types of online speech. Notably, size and market share varies significantly and are not always correlated. For instance, smaller platforms sometimes create larger abuses as shown in the case of online extremism. As technology becomes more user-facing and reaches a wider audience, it becomes more necessary to assess and address the associated risks.</p> <h2 id="content-management-and-multistakeholderism">Content management and multistakeholderism</h2> <p>The second major question is concerned with content management and the roles different actors play in a localised multi-stakeholder approach. Specifically, how should the Guidelines address the legitimate restrictions of content as enshrined in internal instruments of human rights? What should a localised multistakeholder regulatory approach look like?</p> <p>With respect to a limitation on the right to freedom of expression under the International Covenant on Civil and Political Rights (ICCPR), a three-part test is widely used to assess whether such a limitation is justified. The test consists of: (a) the limitation must be provided for by law; (b) it must pursue a legitimate aim; and (c) it must be necessary and proportionate for a legitimate purpose. One concern regarding legitimate restrictions is the varying levels of adaptation and treaty ratification across countries. The issue of legality might also become a slippery slope considering the state efforts for achieving the legality of restriction across the world. Case law plays a crucial role. Citing the <a href="https://globalfreedomofexpression.columbia.edu/cases/new-york-times-co-v-united-states/">Pentagon Papers case</a> as an example, researchers highlighted the potential use of national security as a legitimate purpose that may instead pose threats to speech, particularly targeting journalists and human right defenders.</p> <p>As local governments continue implementing domestic legislation to regulate online speech in the virtual sphere, there is a consensus within the broader content moderation community that a human centred, localised, and multi-stakeholder approach is warranted, which was reflected in community initiatives such as the <a href="https://www.article19.org/wp-content/uploads/2021/10/A19-SMC.pdf">Social Media Council</a> proposed by civil society organisation Article 19. However, there are remaining questions: How do these initiatives gain legitimacy compared to some existing efforts by government agencies and private platforms (e.g., the Oversight Board)? How can we ensure that private platforms endorse these local regulatory systems, if any?</p> <h3 id="emerging-technology">Emerging technology</h3> <p>In terms of future proofing of the Guidelines, it is crucial to ensure that the guidelines are flexible enough to adapt to new and emerging technologies. There are technical challenges that need to be considered. Firstly, the wide use of algorithmic content moderation may bring biases and varying effectiveness in detecting different types of content, which might disproportionately harm underrepresented communities. Moreover, the rise of generative AI also introduces new issues of content moderation where there might be a potential explosion of harmful content that is generated by AI instead of humans.</p> <p>On a broader scale, the challenge we are facing is the ethical adaptation of any regulatory frameworks in line with ever-changing technology. How can we address the normative gaps between offline human rights principles and the needs and interests of online users? Moving beyond, incorporating risk and impact assessments at the design stage becomes crucial to mitigate potential harms arising from future technologies, especially in the case of AI where our basic understanding is still evolving.</p> <h3 id="gender-and-intersectionality">Gender and intersectionality</h3> <p>While people’s social identities can overlap, which could create compounding experiences of discrimination in the digital sphere, how might we address these different kinds of differences when we aim for a more equitable and inclusive moderation design. Are there specific elements that should be considered to ensure the guidelines are sensitive to gender and intersectionality? A case in point is the moderation of pornography, where important considerations arise regarding gender equality and freedom of expression. Historically, marginalised communities including women have been targets of online hate crimes. While copyright law was previously explored as a means to protect against non-consensual explicit content, the rise of generative AI and synthetic media pose new challenges in applying to combat misuse in sexual abuse cases. From a regulatory perspective, balancing these concerns can be extremely challenging especially in determining what falls under obscenity, or prurient interest. It is also crucial to be mindful of not reinforcing gender roles and stereotypes when applying a gender perspective.</p> <h3 id="the-way-ahead">The Way Ahead</h3> <p>The consultation brought together a large number of researchers working across academia, civil society, government and the technical community. Their representative comments will be included in a new report being compiled by the Innovation for Policy Foundation, that will be shared with the UNESCO team developing the Guidelines. The Guidelines will be finalised in the second half of 2023. We anticipate that our input will help to facilitate future research on digital platforms and online speech governance. Look out for further updates on our social media channels. More details regarding the ongoing consultation process of the Guidelines could be found through <a href="https://www.unesco.org/en/internet-conference/guidelines-consultation-process">UNESCO’s website</a>.</p>]]></content><author><name></name></author><category term="commentaries"/><category term="internet"/><category term="trust"/><category term="multistakeholder"/><summary type="html"><![CDATA[Recent responses to an independent community consultation on current UNESCO Guidelines for regulating digital platforms.]]></summary></entry><entry><title type="html">Bridging the Gaps in the Asia-Pacific Information Superhighway</title><link href="https://deeliu97.github.io/blog/2019/the-diplomat/" rel="alternate" type="text/html" title="Bridging the Gaps in the Asia-Pacific Information Superhighway"/><published>2019-09-13T14:14:00+00:00</published><updated>2019-09-13T14:14:00+00:00</updated><id>https://deeliu97.github.io/blog/2019/the-diplomat</id><content type="html" xml:base="https://deeliu97.github.io/blog/2019/the-diplomat/"><![CDATA[<p>Originally published in <a href="https://thediplomat.com/2019/09/bridging-the-gaps-in-the-asia-pacific-information-superhighway/">The Diplomat</a>.</p> <p>In remote rural villages in Bangladesh, women in pink and blue uniforms, known as the Info Ladies, arrive on bicycles bringing a connection to villagers who want to see the faces of their loved ones working overseas.</p> <p><a href="https://www.itu.int/en/ITU-D/Statistics/Pages/stat/default.aspx">Among 163 million people in the country, only 24 million are connected to the web</a>. As the world becomes even more interconnected online, there are still many who cannot access the information they need for daily life.</p> <p>Inclusiveness has been a major concern regarding new technologies. <a href="http://uis.unesco.org/sites/default/files/documents/information-communication-technologies-education-asia-ict-integration-e-readiness-schools-2014-en_0.pdf">The 2014 Information and Communication Technology (ICT) education survey</a> by UNESCO showed that only 7 percent of all public schools in Cambodia were connected to a reliable power source, making it difficult to integrate radios and televisions into the curricula, not to mention computers.</p> <p>“Among the ASEAN countries, Cambodia has the lowest literacy rate, which leads to a huge concern, especially when it comes to digital literacy,” said Nou Keosothea, Deputy Secretary-General of the Office of the Council of Ministers, National Committee for Economic and Social Commission for Asia and the Pacific. “They need to work closely to include all, not only for the young people, but also for the old people.”</p> <p>Besides higher rates of internet usage by younger generations compared with older people, pronounced digital gaps are also seen between more-developed and developing countries, urban and rural areas, different income levels, education, and women and men.</p> <p>The internet will be universally available only if these digital divides are addressed, said Misako Ito, Regional Adviser for Communication and Information at UNESCO Bangkok, while presenting UNESCO’s Internet Universality Indicator Framework at <a href="https://www.unescap.org/our-work/ict-disaster-risk-reduction/asia-pacific-information-superhighway">the Third session of the Asia-Pacific Information Superhighway</a> Steering Committee on August 27.</p> <blockquote> <p>“A new standard becomes important today for advocating an internet ecosystem that works for everyone, not only for those who are in power, but also for the end-users, communities, civil societies, and academia,” Ms Ito said.</p> </blockquote> <p>In 2015, UNESCO adopted the concept of Internet Universality, which highlights four key principles underpinning the growth and evolution of the internet, calling for a cyberspace that is based on human rights (R), that is open (O) and accessible to all (A), and nurtured by multi-stakeholder participation (M).</p> <p>To conduct more concrete assessments at the country level, a research framework of indicators was structured around the ROAM principles, with the addition of Cross-Cutting (X) indicators concerning gender perspectives, the needs of children, sustainable development, trust and security, and legal and ethical issues. The framework, collectively known as the ROAM-X indicator Framework, allows governments and other contributors to map national internet ecosystems and figure out possible enhancements both in policy and practice.</p> <blockquote> <p>“I can see a clear framework behind the project implementation,” Nou Keosothea said. “In countries like Cambodia who lack capacities to implement the internet strategies, we need a guideline as a tool to comply with.”</p> </blockquote> <p>As the internet becomes more pervasive, policymakers need to address relevant risks and concerns. For most countries in this region, especially those island countries in the Pacific, internet accessibility is at a beginning stage but growing everywhere, said Kisione Finau, the Director of Information Technology Services (ITS) at the University of the South Pacific in Suva Fiji. “But for internet universality, it’s still a huge task.”</p> <blockquote> <p>“About 15 countries in the Pacific have submarine fiber, with seven countries expecting the construction of submarine cable probably next year,” said Finau. “For most of them, internet connectivity is the highest priority, and the second is cybersecurity.”</p> </blockquote> <p>Cybersecurity relates to the integrity of the network as well as the protection of internet users against fraud and other types of cyberattacks. “Cybercrime becomes prominent in these countries, since the internet might be used as a window to do a lot of illegal activities,” Finau said.</p> <p>Other concerns emerging from the complex internet environment include protection of human rights, cultural diversity and ethical issues, misinformation on social media and child protection.</p> <blockquote> <p>“Age-appropriate is important. Regarding the access to the internet and the information that is harmful to especially kids, we have to be careful,” said Javad Momeni, Director of the Division for UNESCO and International Scientific Cooperation Department of Sustainable Development in the Ministry of Foreign Affairs of the Islamic Republic of Iran. “Other dimensions, including cultural diversity and ethical issues, also need attention.”</p> </blockquote> <p>Following online consultations with more than 2,000 experts and 66 national governments, along with consultative meetings in 32 countries, pilot evaluations were undertaken in Brazil, Senegal and Thailand between July and September 2018.</p> <p>Based on extensive research and interviews with members of the policy-making, regulatory, industry, academic and civil-society sectors, the pilot report in Thailand exemplified how the ROAM-X indicators framework could help countries to gain a holistic diagnosis of national internet policies, digital environments and structural causes of digital inequalities. To this end, the first multi-stakeholder consultation on the national assessment will be held as part of the Open Tech Summit Thailand 2019 on October 1 and 2.</p> <p>With expressions of interest from countries in this region during the Third Session of the AP-IS Steering Committee, discussions are ongoing about the potential of the internet to enable the ROAM principles highlighted in the UNESCO concept of Internet Universality such as human rights, empower individuals and communities, and facilitate sustainable development.</p> <p>Liu Diyi is a master student at the School of Journalism and Communication at Renmin University of China and former intern at UNESCO Bangkok.</p>]]></content><author><name></name></author><category term="commentaries"/><category term="internet"/><category term="asia-pacific"/><summary type="html"><![CDATA[How can we foster a more connected and inclusive Asia-Pacific?]]></summary></entry></feed>